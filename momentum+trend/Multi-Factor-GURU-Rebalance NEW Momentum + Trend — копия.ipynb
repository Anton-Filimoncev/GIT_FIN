{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests \n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from pandas_datareader.famafrench import get_available_datasets\n",
    "import pandas_datareader.data as web\n",
    "from yahoo_fin import stock_info as si\n",
    "from performance_analysis import annualized_return\n",
    "from performance_analysis import annualized_standard_deviation\n",
    "from performance_analysis import max_drawdown\n",
    "from performance_analysis import gain_to_pain_ratio\n",
    "from performance_analysis import calmar_ratio\n",
    "from performance_analysis import sharpe_ratio\n",
    "from performance_analysis import sortino_ratio\n",
    "import yfinance as yf\n",
    "import apiclient.discovery\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import httplib2\n",
    "from sklearn import mixture as mix\n",
    "import seaborn as sns \n",
    "import gspread as gd\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NSE:AARTIIND', 'NSE:ACC', 'NSE:ADANIENT', 'NSE:ADANIPORTS', 'NSE:AMARAJABAT', 'NSE:AMBUJACEM', 'NSE:APOLLOHOSP', 'NSE:APOLLOTYRE', 'NSE:ASHOKLEY', 'NSE:ASIANPAINT', 'NSE:AUROPHARMA', 'NSE:AXISBANK', 'NSE:BAJAJ-AUTO', 'NSE:BAJFINANCE', 'NSE:BAJAJFINSV', 'NSE:BALKRISIND', 'NSE:BANDHANBNK', 'NSE:BANKBARODA', 'NSE:BATAINDIA', 'NSE:BERGEPAINT', 'NSE:BEL', 'NSE:BHARATFORG', 'NSE:BHEL', 'NSE:BPCL', 'NSE:BHARTIARTL', 'NSE:BIOCON', 'NSE:BOSCHLTD', 'NSE:BRITANNIA', 'NSE:CADILAHC', 'NSE:CANBK', 'NSE:CHOLAFIN', 'NSE:CIPLA', 'NSE:COALINDIA', 'NSE:COFORGE', 'NSE:COLPAL', 'NSE:CONCOR', 'NSE:CUMMINSIND', 'NSE:DABUR', 'NSE:DIVISLAB', 'NSE:DLF', 'NSE:LALPATHLAB', 'NSE:DRREDDY', 'NSE:EICHERMOT', 'NSE:ESCORTS', 'NSE:EXIDEIND', 'NSE:GAIL', 'NSE:GLENMARK', 'NSE:GMRINFRA', 'NSE:GODREJCP', 'NSE:GODREJPROP', 'NSE:GRASIM', 'NSE:HAVELLS', 'NSE:HCLTECH', 'NSE:HDFCAMC', 'NSE:HDFCBANK', 'NSE:HDFCLIFE', 'NSE:HEROMOTOCO', 'NSE:HINDALCO', 'NSE:HINDPETRO', 'NSE:HINDUNILVR', 'NSE:HDFC', 'NSE:ICICIBANK', 'NSE:ICICIGI', 'NSE:ICICIPRULI', 'NSE:IDFCFIRSTB', 'NSE:IBULHSGFIN', 'NSE:IOC', 'NSE:IGL', 'NSE:INDUSTOWER', 'NSE:INDUSINDBK', 'NSE:NAUKRI', 'NSE:INFY', 'NSE:INDIGO', 'NSE:ITC', 'NSE:JINDALSTEL', 'NSE:JSWSTEEL', 'NSE:JUBLFOOD', 'NSE:KOTAKBANK', 'NSE:L&TFH', 'NSE:LT', 'NSE:LICHSGFIN', 'NSE:LUPIN', 'NSE:MGL', 'NSE:M&MFIN', 'NSE:M&M', 'NSE:MANAPPURAM', 'NSE:MARICO', 'NSE:MARUTI', 'NSE:MFSL', 'NSE:MINDTREE', 'NSE:MOTHERSUMI', 'NSE:MRF', 'NSE:MUTHOOTFIN', 'NSE:NATIONALUM', 'NSE:NESTLEIND', 'NSE:NMDC', 'NSE:NTPC', 'NSE:ONGC', 'NSE:PAGEIND', 'NSE:PETRONET', 'NSE:PIDILITIND', 'NSE:PEL', 'NSE:PFC', 'NSE:POWERGRID', 'NSE:PNB', 'NSE:PVR', 'NSE:RBLBANK', 'NSE:RECLTD', 'NSE:RELIANCE', 'NSE:SBILIFE', 'NSE:SHREECEM', 'NSE:SRTRANSFIN', 'NSE:SIEMENS', 'NSE:SRF', 'NSE:SBIN', 'NSE:SAIL', 'NSE:SUNPHARMA', 'NSE:SUNTV', 'NSE:TATACHEM', 'NSE:TCS', 'NSE:TATACONSUM', 'NSE:TATAMOTORS', 'NSE:TATAPOWER', 'NSE:TATASTEEL', 'NSE:TECHM', 'NSE:FEDERALBNK', 'NSE:RAMCOCEM', 'NSE:TITAN', 'NSE:TORNTPHARM', 'NSE:TORNTPOWER', 'NSE:TVSMOTOR', 'NSE:ULTRACEMCO', 'NSE:UBL', 'NSE:MCDOWELL-N', 'NSE:UPL', 'NSE:VEDL', 'NSE:IDEA', 'NSE:VOLTAS', 'NSE:WIPRO', 'NSE:ZEEL', 'NSE:ALKEM', 'NSE:AUBANK', 'NSE:DEEPAKNTR', 'NSE:IRCTC', 'NSE:NAM-INDIA', 'NSE:LTI', 'NSE:NAVINFLUOR', 'NSE:PFIZER', 'NSE:PIIND', 'NSE:TRENT', 'NSE:APLLTD', 'NSE:CUB', 'NSE:GRANULES', 'NSE:GUJGASLTD', 'NSE:LTTS', 'NSE:MPHASIS']\n"
     ]
    }
   ],
   "source": [
    "# подключение к гугл таблице\n",
    "\n",
    "LIST = 'Индия'\n",
    "\n",
    "index = '^AXJO'\n",
    "\n",
    "exchange = 'NSE:'\n",
    "\n",
    "exchange_yahoo =  '.NS'\n",
    "\n",
    "# tickers = si.tickers_sp500()\n",
    "cheked_year = '2015' \n",
    "cheked_year_end = '2020' \n",
    "\n",
    "\n",
    "# Файл, полученный в Google Developer Console\n",
    "CREDENTIALS_FILE = 'Seetzzz-1cb93f64d8d7.json'\n",
    "# ID Google Sheets документа (можно взять из его URL)\n",
    "spreadsheet_id = '1lDhu6-tBmoh66a1mY3RU2yPV2_3uIzNSQWNI5UtMcag'\n",
    "spreadsheet_id2 = '1A3leW6ZfsoVEPXZsv0Loj4eAbyKRchnHrJLdP4RIXDA'\n",
    "#\n",
    "# Авторизуемся и получаем service — экземпляр доступа к API\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    CREDENTIALS_FILE,\n",
    "    ['https://www.googleapis.com/auth/spreadsheets',\n",
    "     'https://www.googleapis.com/auth/drive'])\n",
    "httpAuth = credentials.authorize(httplib2.Http())\n",
    "service = apiclient.discovery.build('sheets', 'v4', http=httpAuth)\n",
    "\n",
    "# ____________________________Парсим тикеры !!!!С ТАБЛИЦЫ!!!! и работаем с ними _______________________________________\n",
    "\n",
    "# Чтения файла\n",
    "values = service.spreadsheets().values().get(\n",
    "    spreadsheetId=spreadsheet_id,\n",
    "    range=f'{LIST}!A1:AA1000',\n",
    "    majorDimension='COLUMNS'\n",
    ").execute()\n",
    "\n",
    "tickers = values['values'][2][1:]\n",
    "\n",
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  156 of 156 completed\n"
     ]
    }
   ],
   "source": [
    "yahoo_ticker_list_full = []\n",
    "\n",
    "for tic in tickers:\n",
    "    yahoo_ticker_list_full.append(tic.replace(exchange,'')+exchange_yahoo )\n",
    "\n",
    "    \n",
    "price_yahoo_pre_main = yf.download(yahoo_ticker_list_full)\n",
    "price_yahoo_pre_main = price_yahoo_pre_main['Adj Close'].fillna(method='ffill').fillna(0)\n",
    "\n",
    "company_yahoo_found = price_yahoo_pre_main.sum()[(price_yahoo_pre_main.sum()!=0)].index.tolist()\n",
    "\n",
    "tickers = []\n",
    "# tickers_ttt = \n",
    "for y_comp in company_yahoo_found:\n",
    "    tickers.append(exchange + y_comp.replace(exchange_yahoo, ''))\n",
    "# price_yahoo_main = yf.download(company_yahoo_found)\n",
    "# price_yahoo_main = price_yahoo_main['Adj Close'].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Читаем данные из огурцов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ['annuals']\n",
    "\n",
    "Data_for_Portfolio_TOTAL = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for ticker in tickers:\n",
    "    with open(f'''C:/Users/Anton/Desktop/Backtesting/BANKA/data_json_{ticker.replace(exchange, '')}.pickle''', 'rb') as f:\n",
    "        data_json = pickle.load(f)\n",
    "\n",
    "    with open(f'''C:/Users/Anton/Desktop/Backtesting/BANKA/data_json_keyratios_{ticker.replace(exchange, '')}.pickle''', 'rb') as f:\n",
    "        data_json_keyratios = pickle.load(f)\n",
    "\n",
    "#     print(data_json)    \n",
    "    try:\n",
    "        date_list = pd.Series(data_json['financials']['annuals']['Fiscal Year'])\n",
    "        keyratios = pd.DataFrame(data_json_keyratios['Fundamental'], index=[0]) \n",
    "        income_df = pd.DataFrame(data_json['financials']['annuals']['income_statement']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('-', 0).replace('', 0).replace('N/A', 0).astype(float)\n",
    "        balance_df = pd.DataFrame(data_json['financials']['annuals']['balance_sheet']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        cashflow_df = pd.DataFrame(data_json['financials']['annuals']['cashflow_statement']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        valuation_ratios_df = pd.DataFrame(data_json['financials']['annuals']['valuation_ratios']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        valuation_and_quality_df = pd.DataFrame(data_json['financials']['annuals']['valuation_and_quality']).set_index(date_list).drop(['Restated Filing Date', 'Filing Date', 'Earnings Release Date'], axis=1).replace('', 0).replace('No Debt', 0).replace('At Loss', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        common_size_ratios_df = pd.DataFrame(data_json['financials']['annuals']['common_size_ratios']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).replace('Negative Tangible Equity', 0).astype(float)\n",
    "        # per_share_data_array_df = pd.DataFrame(data_json['financials']['annuals']['per_share_data_array']).set_index(date_list).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        per_share_data_df = pd.DataFrame(data_json['financials']['annuals']['per_share_data_array']).set_index(date_list).replace('', 0).replace('No Debt', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        \n",
    "        check = 1\n",
    "    except:\n",
    "        check = 0\n",
    "        print(ticker)\n",
    "        \n",
    "        pass\n",
    "            \n",
    "    if check == 1:\n",
    "    \n",
    "        try:\n",
    "            Data_for_Portfolio = pd.DataFrame()\n",
    "        #     \n",
    "            Data_for_Portfolio['E/P'] = income_df['Net Income'] / valuation_and_quality_df['Market Cap'] \n",
    "            # income_df['Net Income']\n",
    "            # valuation_and_quality_df.replace('-', 0)\n",
    "\n",
    "\n",
    "#             Data_for_Portfolio['EBITDA/EV'] = income_df['EBITDA'] / (valuation_and_quality_df['Enterprise Value ($M)']*1000000)\n",
    "            \n",
    "            Data_for_Portfolio['EBITDA/EV'] = 1/valuation_ratios_df['EV-to-EBITDA']\n",
    "\n",
    "\n",
    "\n",
    "            Data_for_Portfolio['ncfcommon'] = cashflow_df['Free Cash Flow']/valuation_and_quality_df['Shares Outstanding (EOP)']\n",
    "\n",
    "            Data_for_Portfolio['Total Debt'] = (balance_df['Short-Term Debt'] + balance_df['Long-Term Debt'])\n",
    "\n",
    "\n",
    "            Data_for_Portfolio['Shareholder Yield'] = -((Data_for_Portfolio['Total Debt'] + cashflow_df['Free Cash Flow'] \\\n",
    "                                             + Data_for_Portfolio['ncfcommon']) / valuation_and_quality_df['Market Cap'])\n",
    "\n",
    "\n",
    "            Data_for_Portfolio['FCF/P'] = cashflow_df['Free Cash Flow'] / valuation_and_quality_df['Market Cap']\n",
    "            Data_for_Portfolio\n",
    "            # Data_for_Portfolio['Shareholder Yield'] \n",
    "            Data_for_Portfolio['Book Value per Share'] = per_share_data_df['Book Value per Share']\n",
    "            Data_for_Portfolio['Dividends per Share'] =per_share_data_df['Dividends per Share'] \n",
    "            Data_for_Portfolio['Dividend Payout Ratio'] =common_size_ratios_df['Dividend Payout Ratio']\n",
    "\n",
    "            Data_for_Portfolio['FCF/Assets'] = cashflow_df['Free Cash Flow'] / balance_df['Total Current Assets']\n",
    "\n",
    "            #Can you generate returns on investment?   \n",
    "\n",
    "\n",
    "\n",
    "            Data_for_Portfolio['ROA'] = common_size_ratios_df['ROA %']  \n",
    "            Data_for_Portfolio['ROIC'] = common_size_ratios_df['ROIC %']\n",
    "            #Do you have a defendable business model?\n",
    "\n",
    "            Data_for_Portfolio['GROSS MARGIN'] = common_size_ratios_df['Gross Margin %']\n",
    "\n",
    "            Data_for_Portfolio['CURRENT RATIO'] = balance_df['Total Current Assets'] / balance_df['Total Current Liabilities']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            Data_for_Portfolio['INTEREST/EBITDA'] = income_df['Interest Expense'] / income_df['EBITDA']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            Data_for_Portfolio['Company'] = ticker\n",
    "            Data_for_Portfolio['Date'] = list(date_list)\n",
    "\n",
    "            Data_for_Portfolio = Data_for_Portfolio.replace([np.inf, -np.inf], 0)\n",
    "            Data_for_Portfolio = Data_for_Portfolio.set_index('Company')\n",
    "            Data_for_Portfolio = Data_for_Portfolio[::-1]\n",
    "\n",
    "            sumz_frame= [Data_for_Portfolio, Data_for_Portfolio_TOTAL]\n",
    "            Data_for_Portfolio_TOTAL = pd.concat(sumz_frame)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "#         print(list(date_list))\n",
    "# list(set(Data_for_Portfolio_TOTAL.index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(Data_for_Portfolio_TOTAL.index.tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(set(Data_for_Portfolio_TOTAL.index.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # ['annuals']\n",
    "\n",
    "# tickers = si.tickers_sp500()\n",
    "\n",
    "# Data_for_Portfolio_TOTAL = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "# for ticker in tickers:\n",
    "#     with open(f'BANKA/data_json_{ticker}.pickle', 'rb') as f:\n",
    "#         data_json = pickle.load(f)\n",
    "\n",
    "#     with open(f'BANKA/data_json_keyratios_{ticker}.pickle', 'rb') as f:\n",
    "#         data_json_keyratios = pickle.load(f)\n",
    "\n",
    "# #     print(data_json)    \n",
    "#     try:\n",
    "#         date_list = pd.Series(data_json['financials']['annuals']['Fiscal Year'])\n",
    "#         keyratios = pd.DataFrame(data_json_keyratios['Fundamental'], index=[0]) \n",
    "#         income_df = pd.DataFrame(data_json['financials']['annuals']['income_statement']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('-', 0).replace('', 0).replace('N/A', 0).astype(float)\n",
    "#         balance_df = pd.DataFrame(data_json['financials']['annuals']['balance_sheet']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "#         cashflow_df = pd.DataFrame(data_json['financials']['annuals']['cashflow_statement']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "#         valuation_ratios_df = pd.DataFrame(data_json['financials']['annuals']['valuation_ratios']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "#         valuation_and_quality_df = pd.DataFrame(data_json['financials']['annuals']['valuation_and_quality']).set_index(date_list).drop(['Restated Filing Date', 'Filing Date', 'Earnings Release Date'], axis=1).replace('', 0).replace('No Debt', 0).replace('At Loss', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "#         common_size_ratios_df = pd.DataFrame(data_json['financials']['annuals']['common_size_ratios']).set_index(date_list).replace('No Debt', 0).replace('At Loss', 0).replace('', 0).replace('-', 0).replace('N/A', 0).replace('Negative Tangible Equity', 0).astype(float)\n",
    "#         # per_share_data_array_df = pd.DataFrame(data_json['financials']['annuals']['per_share_data_array']).set_index(date_list).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "#         per_share_data_df = pd.DataFrame(data_json['financials']['annuals']['per_share_data_array']).set_index(date_list).replace('', 0).replace('No Debt', 0).replace('-', 0).replace('N/A', 0).astype(float)\n",
    "        \n",
    "#         check = 1\n",
    "#     except:\n",
    "#         pass\n",
    "            \n",
    "#     if check == 1:\n",
    "    \n",
    "#         Data_for_Portfolio = pd.DataFrame()\n",
    "#     #     \n",
    "#         Data_for_Portfolio['E/P'] = income_df['Net Income'] / valuation_and_quality_df['Market Cap'] \n",
    "#         # income_df['Net Income']\n",
    "#         # valuation_and_quality_df.replace('-', 0)\n",
    "#         Data_for_Portfolio\n",
    "#         try:\n",
    "#             Data_for_Portfolio['EBITDA/EV'] = income_df['EBITDA'] / (valuation_and_quality_df['Enterprise Value ($M)']*1000000)\n",
    "#         except:\n",
    "#             Data_for_Portfolio['EBITDA/EV'] = Data_for_Portfolio['E/P'] * 0\n",
    "\n",
    "#         Data_for_Portfolio['ncfcommon'] = cashflow_df['Free Cash Flow']/valuation_and_quality_df['Shares Outstanding (EOP)']\n",
    "#         try:\n",
    "#             Data_for_Portfolio['Total Debt'] = (balance_df['Short-Term Debt'] + balance_df['Long-Term Debt'])\n",
    "#         except:\n",
    "#             Data_for_Portfolio['Total Debt'] = Data_for_Portfolio['ncfcommon'] * 0\n",
    "# #             print(Data_for_Portfolio)\n",
    "\n",
    "#         Data_for_Portfolio['Shareholder Yield'] = -((Data_for_Portfolio['Total Debt'] + cashflow_df['Free Cash Flow'] \\\n",
    "#                                          + Data_for_Portfolio['ncfcommon']) / valuation_and_quality_df['Market Cap'])\n",
    "\n",
    "\n",
    "#         Data_for_Portfolio['FCF/P'] = cashflow_df['Free Cash Flow'] / valuation_and_quality_df['Market Cap']\n",
    "#         Data_for_Portfolio\n",
    "#         # Data_for_Portfolio['Shareholder Yield'] \n",
    "#         Data_for_Portfolio['Book Value per Share'] = per_share_data_df['Book Value per Share']\n",
    "#         Data_for_Portfolio['Dividends per Share'] =per_share_data_df['Dividends per Share'] \n",
    "#         Data_for_Portfolio['Dividend Payout Ratio'] =common_size_ratios_df['Dividend Payout Ratio']\n",
    "\n",
    "#         try:\n",
    "#             Data_for_Portfolio['FCF/Assets'] = cashflow_df['Free Cash Flow'] / balance_df['Total Current Assets']\n",
    "#         except:\n",
    "#             Data_for_Portfolio['FCF/Assets'] = cashflow_df['Free Cash Flow'] / balance_df['Total Assets']\n",
    "#         #Can you generate returns on investment?   \n",
    "\n",
    "\n",
    "\n",
    "#         Data_for_Portfolio['ROA'] = common_size_ratios_df['ROA %']  \n",
    "#         Data_for_Portfolio['ROIC'] = common_size_ratios_df['ROIC %']\n",
    "#         #Do you have a defendable business model?\n",
    "#         try:\n",
    "#             Data_for_Portfolio['GROSS MARGIN'] = common_size_ratios_df['Gross Margin %']\n",
    "#         except:\n",
    "#             Data_for_Portfolio['GROSS MARGIN'] = Data_for_Portfolio['ROA'] * 0\n",
    "#         #Current Financial Strength\n",
    "#         try:\n",
    "#             Data_for_Portfolio['CURRENT RATIO'] = balance_df['Total Current Assets'] / balance_df['Total Current Liabilities']\n",
    "#         except:\n",
    "#             Data_for_Portfolio['CURRENT RATIO'] = balance_df['Total Assets'] / balance_df['Total Liabilities']\n",
    "\n",
    "\n",
    "#         try:\n",
    "#             Data_for_Portfolio['INTEREST/EBITDA'] = income_df['Interest Expense'] / income_df['EBITDA']\n",
    "#         except:\n",
    "#             try:\n",
    "#                 Data_for_Portfolio['INTEREST/EBITDA'] = income_df['Interest Expense (Positive)'] / income_df['EBITDA']\n",
    "#             except:\n",
    "#                 Data_for_Portfolio['INTEREST/EBITDA'] = income_df['Interest Expense'] / 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         Data_for_Portfolio['Company'] = ticker\n",
    "#         Data_for_Portfolio['Date'] = list(date_list)\n",
    "\n",
    "#         Data_for_Portfolio = Data_for_Portfolio.replace([np.inf, -np.inf], 0)\n",
    "#         Data_for_Portfolio = Data_for_Portfolio.set_index('Company')\n",
    "#         Data_for_Portfolio = Data_for_Portfolio[::-1]\n",
    "\n",
    "#         sumz_frame= [Data_for_Portfolio, Data_for_Portfolio_TOTAL]\n",
    "#         Data_for_Portfolio_TOTAL = pd.concat(sumz_frame)\n",
    "\n",
    "\n",
    "\n",
    "# #         print(list(date_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "E/P                       0.02215\n",
       "EBITDA/EV                0.045106\n",
       "ncfcommon                4.400025\n",
       "Total Debt                    9.0\n",
       "Shareholder Yield       -0.011407\n",
       "FCF/P                    0.011371\n",
       "Book Value per Share       50.017\n",
       "Dividends per Share          2.25\n",
       "Dividend Payout Ratio       0.263\n",
       "FCF/Assets               0.077923\n",
       "ROA                        10.989\n",
       "ROIC                        16.98\n",
       "GROSS MARGIN               55.296\n",
       "CURRENT RATIO            3.984498\n",
       "INTEREST/EBITDA         -0.092927\n",
       "Date                      2016-03\n",
       "Name: NSE:ZEEL, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Data_for_Portfolio_TOTAL.loc[ticker].fillna(0).iloc[int(cheked_year_end) - (int(cheked_year)-1+0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  156 of 156 completed\n",
      "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:169: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  Filter_Vol_Signal = Sector_stock_rolling_vol[str(int(start)+i)].dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- NAM-INDIA.NS: Data doesn't exist for startDate = 1356987600, endDate = 1451595600\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- MGL.NS: Data doesn't exist for startDate = 1356987600, endDate = 1451595600\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- LTTS.NS: Data doesn't exist for startDate = 1356987600, endDate = 1451595600\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- LTI.NS: Data doesn't exist for startDate = 1356987600, endDate = 1451595600\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- INDUSTOWER.NS: No data found for this date range, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- HDFCAMC.NS: Data doesn't exist for startDate = 1356987600, endDate = 1451595600\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:313: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  prices_yearly_signal = np.where(prices_yearly_returns[str(int(cheked_year)+i)].iloc[-1] > 0, 1, 0)\n",
      "<ipython-input-22-7b8f81555159>:350: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  cum_str_returns_bh = (price_yahoo[yahoo_ticker_list][str(int(start)+i+1)].fillna(method='backfill').pct_change() + 1).cumprod().fillna(0)\n",
      "<ipython-input-22-7b8f81555159>:381: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  profit_yah = price_yahoo_main[top_rated_company_yahoo][str(int(start)+i+1)].fillna(method='backfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Год начальный\n",
      "2016-1-1\n",
      "[3.7923503101334792]\n",
      "top_rated_company\n",
      "['NSE:SUNTV', 'NSE:APLLTD', 'NSE:TORNTPHARM', 'NSE:DIVISLAB', 'NSE:LALPATHLAB', 'NSE:TCS', 'NSE:HDFCAMC', 'NSE:COLPAL', 'NSE:COFORGE', 'NSE:TORNTPOWER', 'NSE:BAJAJ-AUTO', 'NSE:INFY', 'NSE:HINDUNILVR', 'NSE:HCLTECH', 'NSE:COALINDIA', 'NSE:HAVELLS', 'NSE:NAM-INDIA', 'NSE:PIDILITIND', 'NSE:RAMCOCEM', 'NSE:HEROMOTOCO', 'NSE:PAGEIND', 'NSE:BALKRISIND', 'NSE:INDIGO', 'NSE:APOLLOTYRE']\n",
      "low_rated_company\n",
      "['NSE:APOLLOHOSP', 'NSE:BHARTIARTL', 'NSE:GAIL', 'NSE:TRENT', 'NSE:JUBLFOOD', 'NSE:LUPIN', 'NSE:JSWSTEEL', 'NSE:RELIANCE', 'NSE:ASHOKLEY', 'NSE:GUJGASLTD', 'NSE:MGL', 'NSE:POWERGRID', 'NSE:IDEA', 'NSE:TATASTEEL', 'NSE:PEL', 'NSE:ADANIENT', 'NSE:LT', 'NSE:GMRINFRA', 'NSE:BHEL', 'NSE:CONCOR', 'NSE:JINDALSTEL', 'NSE:PFC', 'NSE:VEDL', 'NSE:SAIL']\n",
      "Max DD\n",
      "[-48.453773534635594]\n",
      "-48.453773534635594\n",
      "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:169: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  Filter_Vol_Signal = Sector_stock_rolling_vol[str(int(start)+i)].dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- NAM-INDIA.NS: Data doesn't exist for startDate = 1388523600, endDate = 1483218000\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- IRCTC.NS: Data doesn't exist for startDate = 1388523600, endDate = 1483218000\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- INDUSTOWER.NS: No data found for this date range, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- HDFCAMC.NS: Data doesn't exist for startDate = 1388523600, endDate = 1483218000\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:313: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  prices_yearly_signal = np.where(prices_yearly_returns[str(int(cheked_year)+i)].iloc[-1] > 0, 1, 0)\n",
      "<ipython-input-22-7b8f81555159>:350: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  cum_str_returns_bh = (price_yahoo[yahoo_ticker_list][str(int(start)+i+1)].fillna(method='backfill').pct_change() + 1).cumprod().fillna(0)\n",
      "<ipython-input-22-7b8f81555159>:381: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  profit_yah = price_yahoo_main[top_rated_company_yahoo][str(int(start)+i+1)].fillna(method='backfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Год начальный\n",
      "2017-1-1\n",
      "[3.7923503101334792, 37.34537337227153]\n",
      "top_rated_company\n",
      "['NSE:SUNTV', 'NSE:LALPATHLAB', 'NSE:TCS', 'NSE:DIVISLAB', 'NSE:LTI', 'NSE:COFORGE', 'NSE:GRASIM', 'NSE:TATACHEM', 'NSE:ZEEL', 'NSE:HEROMOTOCO', 'NSE:INFY', 'NSE:HCLTECH', 'NSE:ITC', 'NSE:BAJAJ-AUTO', 'NSE:HINDUNILVR', 'NSE:MARICO', 'NSE:MPHASIS', 'NSE:NAM-INDIA', 'NSE:PAGEIND', 'NSE:PIDILITIND', 'NSE:PIIND', 'NSE:HDFCAMC', 'NSE:IGL', 'NSE:COLPAL']\n",
      "low_rated_company\n",
      "['NSE:GMRINFRA', 'NSE:TITAN', 'NSE:GUJGASLTD', 'NSE:APOLLOHOSP', 'NSE:TVSMOTOR', 'NSE:MCDOWELL-N', 'NSE:UBL', 'NSE:IRCTC', 'NSE:M&M', 'NSE:PVR', 'NSE:LT', 'NSE:PEL', 'NSE:GODREJPROP', 'NSE:RELIANCE', 'NSE:JINDALSTEL', 'NSE:DRREDDY', 'NSE:TATASTEEL', 'NSE:DEEPAKNTR', 'NSE:JUBLFOOD', 'NSE:BHEL', 'NSE:BHARTIARTL', 'NSE:ADANIENT', 'NSE:IDEA', 'NSE:SAIL']\n",
      "Max DD\n",
      "[-48.453773534635594, -46.489660794610465]\n",
      "-48.453773534635594\n",
      "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:169: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  Filter_Vol_Signal = Sector_stock_rolling_vol[str(int(start)+i)].dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- IRCTC.NS: Data doesn't exist for startDate = 1420059600, endDate = 1514754000\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- INDUSTOWER.NS: No data found for this date range, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- HDFCAMC.NS: Data doesn't exist for startDate = 1420059600, endDate = 1514754000\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:313: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  prices_yearly_signal = np.where(prices_yearly_returns[str(int(cheked_year)+i)].iloc[-1] > 0, 1, 0)\n",
      "<ipython-input-22-7b8f81555159>:350: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  cum_str_returns_bh = (price_yahoo[yahoo_ticker_list][str(int(start)+i+1)].fillna(method='backfill').pct_change() + 1).cumprod().fillna(0)\n",
      "<ipython-input-22-7b8f81555159>:381: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  profit_yah = price_yahoo_main[top_rated_company_yahoo][str(int(start)+i+1)].fillna(method='backfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Год начальный\n",
      "2018-1-1\n",
      "[3.7923503101334792, 37.34537337227153, 8.537300736198041]\n",
      "top_rated_company\n",
      "['NSE:SUNTV', 'NSE:TCS', 'NSE:NMDC', 'NSE:TATACHEM', 'NSE:LALPATHLAB', 'NSE:COLPAL', 'NSE:NAM-INDIA', 'NSE:INFY', 'NSE:HEROMOTOCO', 'NSE:PAGEIND', 'NSE:HINDUNILVR', 'NSE:NAUKRI', 'NSE:LTI', 'NSE:NATIONALUM', 'NSE:ITC', 'NSE:DIVISLAB', 'NSE:BAJAJ-AUTO', 'NSE:NESTLEIND', 'NSE:ADANIPORTS', 'NSE:PETRONET', 'NSE:EICHERMOT', 'NSE:COFORGE', 'NSE:HCLTECH', 'NSE:JSWSTEEL']\n",
      "low_rated_company\n",
      "['NSE:NTPC', 'NSE:APLLTD', 'NSE:BHARTIARTL', 'NSE:AARTIIND', 'NSE:SUNPHARMA', 'NSE:BIOCON', 'NSE:RELIANCE', 'NSE:APOLLOHOSP', 'NSE:DRREDDY', 'NSE:TRENT', 'NSE:LUPIN', 'NSE:SRF', 'NSE:APOLLOTYRE', 'NSE:PFC', 'NSE:TVSMOTOR', 'NSE:TATAMOTORS', 'NSE:IRCTC', 'NSE:SAIL', 'NSE:GRASIM', 'NSE:LT', 'NSE:GRANULES', 'NSE:DEEPAKNTR', 'NSE:ADANIENT', 'NSE:IDEA']\n",
      "Max DD\n",
      "[-48.453773534635594, -46.489660794610465, -71.50255218972576]\n",
      "-71.50255218972576\n",
      "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:169: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  Filter_Vol_Signal = Sector_stock_rolling_vol[str(int(start)+i)].dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- IRCTC.NS: Data doesn't exist for startDate = 1451595600, endDate = 1546290000\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- INDUSTOWER.NS: No data found for this date range, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:313: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  prices_yearly_signal = np.where(prices_yearly_returns[str(int(cheked_year)+i)].iloc[-1] > 0, 1, 0)\n",
      "<ipython-input-22-7b8f81555159>:350: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  cum_str_returns_bh = (price_yahoo[yahoo_ticker_list][str(int(start)+i+1)].fillna(method='backfill').pct_change() + 1).cumprod().fillna(0)\n",
      "<ipython-input-22-7b8f81555159>:381: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  profit_yah = price_yahoo_main[top_rated_company_yahoo][str(int(start)+i+1)].fillna(method='backfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Год начальный\n",
      "2019-1-1\n",
      "[3.7923503101334792, 37.34537337227153, 8.537300736198041, 20.590634564195984]\n",
      "top_rated_company\n",
      "['NSE:SUNTV', 'NSE:HDFCAMC', 'NSE:NMDC', 'NSE:LALPATHLAB', 'NSE:TCS', 'NSE:NATIONALUM', 'NSE:COALINDIA', 'NSE:COLPAL', 'NSE:LTI', 'NSE:HINDUNILVR', 'NSE:LTTS', 'NSE:NESTLEIND', 'NSE:DIVISLAB', 'NSE:NAM-INDIA', 'NSE:ITC', 'NSE:HCLTECH', 'NSE:MARICO', 'NSE:INFY', 'NSE:MINDTREE', 'NSE:COFORGE', 'NSE:NAUKRI', 'NSE:PAGEIND', 'NSE:TATACHEM', 'NSE:MPHASIS']\n",
      "low_rated_company\n",
      "['NSE:M&M', 'NSE:JINDALSTEL', 'NSE:TRENT', 'NSE:DEEPAKNTR', 'NSE:BPCL', 'NSE:SIEMENS', 'NSE:VOLTAS', 'NSE:IOC', 'NSE:LT', 'NSE:NTPC', 'NSE:RELIANCE', 'NSE:CONCOR', 'NSE:APOLLOTYRE', 'NSE:RAMCOCEM', 'NSE:ASHOKLEY', 'NSE:PFC', 'NSE:PEL', 'NSE:BHEL', 'NSE:TVSMOTOR', 'NSE:GRASIM', 'NSE:BHARTIARTL', 'NSE:IDEA', 'NSE:GMRINFRA', 'NSE:TATAMOTORS']\n",
      "Max DD\n",
      "[-48.453773534635594, -46.489660794610465, -71.50255218972576, -86.81275432444822]\n",
      "-86.81275432444822\n",
      "iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:169: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  Filter_Vol_Signal = Sector_stock_rolling_vol[str(int(start)+i)].dropna()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "\n",
      "1 Failed download:\n",
      "- INDUSTOWER.NS: No data found for this date range, symbol may be delisted\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7b8f81555159>:313: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  prices_yearly_signal = np.where(prices_yearly_returns[str(int(cheked_year)+i)].iloc[-1] > 0, 1, 0)\n",
      "<ipython-input-22-7b8f81555159>:350: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  cum_str_returns_bh = (price_yahoo[yahoo_ticker_list][str(int(start)+i+1)].fillna(method='backfill').pct_change() + 1).cumprod().fillna(0)\n",
      "<ipython-input-22-7b8f81555159>:381: FutureWarning: Indexing a DataFrame with a datetimelike index using a single string to slice the rows, like `frame[string]`, is deprecated and will be removed in a future version. Use `frame.loc[string]` instead.\n",
      "  profit_yah = price_yahoo_main[top_rated_company_yahoo][str(int(start)+i+1)].fillna(method='backfill')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "Год начальный\n",
      "2020-1-1\n",
      "[3.7923503101334792, 37.34537337227153, 8.537300736198041, 20.590634564195984, 36.301752686738205]\n",
      "top_rated_company\n",
      "['NSE:SUNTV', 'NSE:HDFCAMC', 'NSE:LALPATHLAB', 'NSE:TCS', 'NSE:NAM-INDIA', 'NSE:COLPAL', 'NSE:NAVINFLUOR', 'NSE:NMDC', 'NSE:ITC', 'NSE:POWERGRID', 'NSE:NESTLEIND', 'NSE:MGL', 'NSE:DEEPAKNTR', 'NSE:COALINDIA', 'NSE:HINDUNILVR', 'NSE:PETRONET', 'NSE:HEROMOTOCO', 'NSE:LTI', 'NSE:HCLTECH', 'NSE:INFY', 'NSE:TATACHEM', 'NSE:TORNTPOWER', 'NSE:DIVISLAB', 'NSE:LTTS']\n",
      "low_rated_company\n",
      "['NSE:MCDOWELL-N', 'NSE:BIOCON', 'NSE:BOSCHLTD', 'NSE:BHARATFORG', 'NSE:LUPIN', 'NSE:RAMCOCEM', 'NSE:MARUTI', 'NSE:LT', 'NSE:APOLLOTYRE', 'NSE:PFC', 'NSE:ASHOKLEY', 'NSE:TVSMOTOR', 'NSE:BPCL', 'NSE:SAIL', 'NSE:VEDL', 'NSE:NATIONALUM', 'NSE:M&M', 'NSE:BHARTIARTL', 'NSE:HINDPETRO', 'NSE:BHEL', 'NSE:GMRINFRA', 'NSE:IOC', 'NSE:TATAMOTORS', 'NSE:IDEA']\n",
      "Max DD\n",
      "[-48.453773534635594, -46.489660794610465, -71.50255218972576, -86.81275432444822, -73.821168930411]\n",
      "-86.81275432444822\n"
     ]
    }
   ],
   "source": [
    "# tickers = ['AAPL', 'INTC']\n",
    "\n",
    "yahoo_ticker_list_full = []\n",
    "\n",
    "for tic in tickers:\n",
    "    yahoo_ticker_list_full.append(tic.replace(exchange,'')+exchange_yahoo )\n",
    "\n",
    "price_yahoo_main = yf.download(yahoo_ticker_list_full)\n",
    "price_yahoo_main = price_yahoo_main['Adj Close'].fillna(method='backfill')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years_len = int(cheked_year_end) - int(cheked_year)\n",
    "\n",
    "portfolio_profit_final = []\n",
    "index_profit_final = []\n",
    "max_dd_list = []\n",
    "\n",
    "\n",
    "Percentile_split = .2\n",
    "\n",
    "Winsorize_Threshold = .025\n",
    "\n",
    "# for i in range(years_len):\n",
    "for i in range(years_len):\n",
    "    print('i'*50)\n",
    "    print(i)\n",
    "\n",
    "    df_res = pd.DataFrame()\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "\n",
    "            Data_for_Portfolio_tick = Data_for_Portfolio_TOTAL.loc[ticker].fillna(0).iloc[int(cheked_year_end) - (int(cheked_year)-1+i)]\n",
    "#             print(Data_for_Portfolio_tick)\n",
    "            sum_frame = [pd.DataFrame([Data_for_Portfolio_tick]), df_res]\n",
    "            df_res = pd.concat(sum_frame )  \n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    Data_for_Portfolio_master_filter = df_res\n",
    "    \n",
    "    yahoo_ticker_list = []\n",
    "\n",
    "    for tic in Data_for_Portfolio_master_filter.index.tolist():\n",
    "        yahoo_ticker_list.append(tic.replace(exchange,'')+exchange_yahoo )\n",
    "         \n",
    "\n",
    "   #Winsorize the metric data and compress outliers if desired\n",
    "    Data_for_Portfolio_master_filter['E/P Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['E/P'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['EBITDA/EV Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['EBITDA/EV'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['FCF/P Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['FCF/P'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "\n",
    "    #create Z score to normalize the metrics\n",
    "    Data_for_Portfolio_master_filter['E/P Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['E/P Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['EBITDA/EV Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['EBITDA/EV Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['FCF/P Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['FCF/P Winsorized'])\n",
    "\n",
    "    Data_for_Portfolio_master_filter['Valuation Score'] = \\\n",
    "            Data_for_Portfolio_master_filter['E/P Z score'] \\\n",
    "            + Data_for_Portfolio_master_filter['EBITDA/EV Z score']\\\n",
    "            + Data_for_Portfolio_master_filter['FCF/P Z score']\n",
    "\n",
    "    Data_for_Portfolio_master_filter['FCF/P Winsorized']\n",
    "\n",
    "    ###### QUALITY FACTOR ######  \n",
    "    Data_for_Portfolio_master_filter = Data_for_Portfolio_master_filter.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "    Data_for_Portfolio_master_filter['FCF/Assets Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['FCF/Assets'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['ROA Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['ROA'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['ROIC Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['ROIC'], \\\n",
    "                                limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['Gross Margin Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['GROSS MARGIN'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['Current Ratio Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['CURRENT RATIO'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['Interest/EBITDA Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['INTEREST/EBITDA'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "\n",
    "    #create Z score\n",
    "    Data_for_Portfolio_master_filter = Data_for_Portfolio_master_filter.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "\n",
    "    Data_for_Portfolio_master_filter['FCF/Assets Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['FCF/Assets Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['ROA Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['ROA Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['ROIC Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['ROIC Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['Gross Margin Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['Gross Margin Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['Current Ratio Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['Current Ratio Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['Interest/EBITDA Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['Interest/EBITDA Winsorized'])\n",
    "\n",
    "    Data_for_Portfolio_master_filter['Quality Score'] = \\\n",
    "        Data_for_Portfolio_master_filter['FCF/Assets Z score'] \\\n",
    "            + Data_for_Portfolio_master_filter['ROA Z score'] \\\n",
    "            + Data_for_Portfolio_master_filter['ROIC Z score']\\\n",
    "            + Data_for_Portfolio_master_filter['Gross Margin Z score']\\\n",
    "            + Data_for_Portfolio_master_filter['Current Ratio Z score']\\\n",
    "            - Data_for_Portfolio_master_filter['Interest/EBITDA Z score']\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    \n",
    "    ###### SHAREHOLDER YIELD FACTOR #####\n",
    "\n",
    "    Data_for_Portfolio_master_filter = Data_for_Portfolio_master_filter.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    Data_for_Portfolio_master_filter['Shareholder Yield Winsorized'] = \\\n",
    "        stats.mstats.winsorize(Data_for_Portfolio_master_filter['Shareholder Yield'], \\\n",
    "                               limits=Winsorize_Threshold)\n",
    "    Data_for_Portfolio_master_filter['Shareholder Yield Z score'] = \\\n",
    "        stats.zscore(Data_for_Portfolio_master_filter['Shareholder Yield Winsorized'])\n",
    "    Data_for_Portfolio_master_filter['Shareholder Yield Score'] = \\\n",
    "        Data_for_Portfolio_master_filter['Shareholder Yield Z score'] \n",
    "        \n",
    "        \n",
    "        ###### LOW VOLATILITY FACTOR ######\n",
    "\n",
    "    #must have fundamental data from previous factors for price based factors\n",
    "    #as some equities have price data and no fundamental data which should not\n",
    "    #be included\n",
    "\n",
    "    # treasury = 'RGBI.ME'\n",
    "    start = cheked_year\n",
    "    # end = current_date\n",
    "    end =  cheked_year_end\n",
    "\n",
    "#     price_yahoo = yf.download(Data_for_Portfolio_master_filter.index.tolist())\n",
    "    price_yahoo = price_yahoo_main[yahoo_ticker_list]\n",
    "\n",
    "    Sector_stock_returns =  price_yahoo.pct_change()      \n",
    "\n",
    "    #create rolling vol metric for previous 2 years\n",
    "    Sector_stock_rolling_vol = Sector_stock_returns.rolling(252*2).std()\n",
    "\n",
    "    #Choose second to last trading day to look at previous vol   \n",
    "    #Sometimes the dates are off when trying to line up end of quarter and business\n",
    "    #days so to eliminate errors in the for loop I go to day of quarter, shift forward\n",
    "    #a business day and then go back two business days\n",
    "\n",
    "    \n",
    "\n",
    "    Filter_Vol_Signal = Sector_stock_rolling_vol[str(int(start)+i)].dropna()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Filter_Vol_Signal_Sort = Filter_Vol_Signal.sort_values().dropna() # для цикла паска тикеров\n",
    "\n",
    "    #create z score and rank for the Volatility Factor\n",
    "    # frame = { 'Vol': Filter_Vol_Signal} \n",
    "\n",
    "    # Filter_Vol_Signal_df = pd.DataFrame(frame)\n",
    "    \n",
    "    \n",
    "#     print(pd.DataFrame(stats.zscore(Filter_Vol_Signal)).mean())\n",
    "#     print(pd.DataFrame(stats.zscore(Filter_Vol_Signal)))\n",
    "\n",
    "#     Filter_Vol_Signal = Filter_Vol_Signal.fillna(0)\n",
    "\n",
    "    # Filter_Vol_Signal_df['Vol Z Score'] = stats.zscore(Filter_Vol_Signal) \n",
    "    # Filter_Vol_Signal_df = Filter_Vol_Signal_df.reset_index()\n",
    "    # print(Filter_Vol_Signal_df)\n",
    "\n",
    "    \n",
    "    Data_for_Portfolio_master_filter['Vol Z Score'] = pd.DataFrame(stats.zscore(Filter_Vol_Signal)).fillna(0).mean().tolist()\n",
    "#     print(stats.zscore(Filter_Vol_Signal))\n",
    "    # Data_for_Portfolio_master_filter = Data_for_Portfolio_master_filter.merge(Filter_Vol_Signal_df, how = 'inner', on = ['ticker']) \n",
    "\n",
    "    ###### TREND FACTOR #####\n",
    "    \n",
    "    total_trend_score = []\n",
    "    \n",
    "    for tic in yahoo_ticker_list:\n",
    "        try:\n",
    "            df = yf.download(tic, str(int(start)+i-2)+'-1-1', str(int(start)+i+1)+'-1-1')\n",
    "        #     print(df)\n",
    "            df = df[['Open', 'High', 'Low', 'Adj Close']]\n",
    "            df['open'] = df['Open'].shift(1)\n",
    "            df['high'] = df['High'].shift(1)\n",
    "            df['low'] = df['Low'].shift(1)\n",
    "            df['close'] = df['Adj Close'].shift(1)\n",
    "\n",
    "            df = df[['open', 'high', 'low', 'close']]\n",
    "            df = df.dropna()\n",
    "\n",
    "            unsup = mix.GaussianMixture(n_components=4,\n",
    "                                        covariance_type=\"spherical\",\n",
    "                                        n_init=100,\n",
    "                                        random_state=42)\n",
    "            unsup.fit(np.reshape(df, (-1, df.shape[1])))\n",
    "            regime = unsup.predict(np.reshape(df, (-1, df.shape[1])))\n",
    "            df['Return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "            Regimes = pd.DataFrame(regime, columns=['Regime'], index=df.index) \\\n",
    "                .join(df, how='inner') \\\n",
    "                .assign(market_cu_return=df.Return.cumsum()) \\\n",
    "                .reset_index(drop=False) \\\n",
    "                .rename(columns={'index': 'Date'})\n",
    "\n",
    "            order = [0, 1, 2, 3]\n",
    "        #     fig = sns.FacetGrid(data=Regimes, hue='Regime', hue_order=order, aspect=2, height=4)\n",
    "        #     fig.map(plt.scatter, 'Date', 'market_cu_return', s=4).add_legend()\n",
    "        #     plt.show()\n",
    "\n",
    "            mean_for_regime = []\n",
    "            cur_price = df['close'][-1]\n",
    "\n",
    "            total_position = 0\n",
    "\n",
    "            for j in order:\n",
    "                mean_for_regime.append(unsup.means_[j][0])\n",
    "    #             print('Mean for regime %i: '%i,unsup.means_[i][0])\n",
    "        #         print('Co-Variance for regime %i: '%i,(unsup.covariances_[i]))\n",
    "\n",
    "            mean_for_regime = np.sort(mean_for_regime)   \n",
    "            for val in  mean_for_regime:\n",
    "                if cur_price > val:\n",
    "                    total_position += 0.25\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            total_trend_score.append(total_position)\n",
    "    #         print(mean_for_regime)\n",
    "    #         print('cur_price')\n",
    "    #         print(cur_price)\n",
    "    #         print('total_position')\n",
    "    #         print(total_position)\n",
    "        except:\n",
    "            total_trend_score.append(0)\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    Data_for_Portfolio_master_filter['Trend Score'] = total_trend_score\n",
    "    \n",
    "    #This is a very simply way to see how much a stock is in a trend up or down\n",
    "    #You could easily make this more complex/robust but it would cost you in \n",
    "    #execution time\n",
    "#     df_sma_50 = price_yahoo.rolling(50).mean()\n",
    "#     df_sma_100 = price_yahoo.rolling(100).mean()\n",
    "#     df_sma_150 = price_yahoo.rolling(150).mean()\n",
    "#     df_sma_200 = price_yahoo.rolling(200).mean()\n",
    "\n",
    "\n",
    "#     Filter_Trend_Signal_50 = df_sma_50[str(int(cheked_year)+i)]\n",
    "#     Filter_Trend_Signal_100 = df_sma_100[str(int(cheked_year)+i)]\n",
    "#     Filter_Trend_Signal_150 = df_sma_150[str(int(cheked_year)+i)]\n",
    "#     Filter_Trend_Signal_200 = df_sma_200[str(int(cheked_year)+i)]\n",
    "\n",
    "#     Price_Signal = price_yahoo[str(int(cheked_year)+i)]\n",
    "\n",
    "#     SMA_all = pd.DataFrame()\n",
    "#     SMA_50 = pd.DataFrame(np.where(Price_Signal > Filter_Trend_Signal_50,1,0)).mean()\n",
    "#     SMA_100 = pd.DataFrame(np.where(Price_Signal > Filter_Trend_Signal_100,1,0)).mean()\n",
    "#     SMA_150 = pd.DataFrame(np.where(Price_Signal > Filter_Trend_Signal_150,1,0)).mean()\n",
    "#     SMA_200 = pd.DataFrame(np.where(Price_Signal > Filter_Trend_Signal_200,1,0)).mean()\n",
    "\n",
    "#     SMA_all['SMA_50'] = SMA_50\n",
    "#     SMA_all['SMA_100'] = SMA_100\n",
    "#     SMA_all['SMA_150'] = SMA_150\n",
    "#     SMA_all['SMA_200'] = SMA_200\n",
    "#     SMA_all['Trend Score'] = np.mean(SMA_all, axis=1)\n",
    "\n",
    "#     # print(SMA_all)\n",
    "#     Data_for_Portfolio_master_filter['Trend Score'] = np.mean(SMA_all, axis=1).tolist()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ###### MOMENTUM FACTOR #####\n",
    "\n",
    "#     print('tut')\n",
    "    \n",
    "    # tickers_momentum = list(Sector_stock_prices_vol_df_1_wide.columns)\n",
    "    #from the academic literature of 12 months - 1 month momentum \n",
    "#     df_mom_11_months = price_yahoo[str(int(cheked_year)+i)].pct_change(22*11)\n",
    "#     Data_for_Portfolio_master_filter['Momentum Score'] = pd.DataFrame(stats.zscore(df_mom_11_months.iloc[242:])).fillna(0).mean().tolist()\n",
    "#     # Data_for_Portfolio_master_filter = Data_for_Portfolio_master_filter.merge(Filter_MOM_df[['ticker','Momentum Score']], how = 'inner', on = ['ticker'])\n",
    "#     Data_for_Portfolio_master_filter\n",
    "\n",
    "\n",
    "\n",
    "    prices = price_yahoo_main[yahoo_ticker_list].asfreq('BM')\n",
    "    prices_yearly_returns = prices.pct_change(12)\n",
    "    prices_yearly_signal = np.where(prices_yearly_returns[str(int(cheked_year)+i)].iloc[-1] > 0, 1, 0)\n",
    "    Data_for_Portfolio_master_filter['Momentum Score'] = prices_yearly_signal\n",
    "\n",
    "    \n",
    "       \n",
    "    ### Create Composite Score from factors ###\n",
    "\n",
    "    #Because we made all the factors with a z score each factor should have equal\n",
    "    #weight in the composite. You could consider changing the weights based on \n",
    "    #historical statistical significance or whatever else seems reasonable\n",
    "\n",
    "    #This particular scoring system only invests in companies with \n",
    "    #positive trend/momentum after ranking by the other factors\n",
    "\n",
    "    Data_for_Portfolio_master_filter['Total Score'] = Data_for_Portfolio_master_filter['Valuation Score'] +  \\\n",
    "        Data_for_Portfolio_master_filter['Quality Score'] + Data_for_Portfolio_master_filter['Shareholder Yield Score'] +\\\n",
    "        Data_for_Portfolio_master_filter['Momentum Score'] + Data_for_Portfolio_master_filter['Trend Score']\n",
    "    \n",
    "    \n",
    "    \n",
    "    Data_for_Portfolio_master_filter = Data_for_Portfolio_master_filter.sort_values('Total Score', ascending=False)\n",
    "    \n",
    "    top_rated_company = Data_for_Portfolio_master_filter[:int(len(Data_for_Portfolio_master_filter) \\\n",
    "                                                              * Percentile_split)].index.tolist()\n",
    "    top_rated_company\n",
    "\n",
    "    low_rated_company = Data_for_Portfolio_master_filter[-int(len(Data_for_Portfolio_master_filter) \\\n",
    "                                                              * Percentile_split):].index.tolist()\n",
    "    low_rated_company\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_hayoo = str(int(start)+i+1)+'-1-1'\n",
    "    end_hayoo = str(int(start)+i+2)+'-1-1'\n",
    "    \n",
    "    cum_str_returns_bh = (price_yahoo[yahoo_ticker_list][str(int(start)+i+1)].fillna(method='backfill').pct_change() + 1).cumprod().fillna(0)\n",
    "    running_max_BH = np.maximum.accumulate(cum_str_returns_bh[1:].fillna(method='backfill') )\n",
    "    drawdown_BH = (cum_str_returns_bh[1:])/running_max_BH - 1\n",
    "    max_dd = drawdown_BH.min()*100\n",
    "\n",
    "\n",
    "    try:\n",
    "        Data_for_Portfolio_master_filter['Max DD'] = max_dd.values \n",
    "    except:\n",
    "        Data_for_Portfolio_master_filter['Max DD'] = [max_dd]\n",
    "    \n",
    "    max_dd_list.append(max_dd.min())\n",
    "    \n",
    "#     # == Доходность\n",
    "    \n",
    "    portfolio_profit = [] \n",
    "    profit_list_index = 0\n",
    "\n",
    "    top_rated_company_yahoo = []\n",
    "    low_rated_company_yahoo = []\n",
    "    \n",
    "    \n",
    "    for tic in top_rated_company:\n",
    "        top_rated_company_yahoo.append(tic.replace(exchange,'')+exchange_yahoo )\n",
    "        \n",
    "    for tic in low_rated_company:\n",
    "        low_rated_company_yahoo.append(tic.replace(exchange,'')+exchange_yahoo )\n",
    "        \n",
    "  \n",
    "    \n",
    "#     profit_yah = yf.download(top_rated_company, start_hayoo, end_hayoo)['Adj Close'].fillna(method='backfill') \n",
    "    profit_yah = price_yahoo_main[top_rated_company_yahoo][str(int(start)+i+1)].fillna(method='backfill') \n",
    "    profit = (profit_yah.iloc[-1]-profit_yah.iloc[0])/profit_yah.iloc[0]\n",
    "    profit = profit.dropna()\n",
    "#     profit_yah['profit'] = profit\n",
    "    portfolio_profit = profit.values.tolist()\n",
    "#     print(profit_yah)\n",
    "            \n",
    "    \n",
    "#     for company in top_rated_company:\n",
    "# #         print('tut1')\n",
    "#         try:\n",
    "#             profit_yah = yf.download(company, start_hayoo, end_hayoo)['Adj Close'].fillna(method='backfill') \n",
    "#             profit = (profit_yah[-1]-profit_yah[0])/profit_yah[0]\n",
    "#             (1 + profit).cumprod()[-1]\n",
    "#             portfolio_profit.append(profit)\n",
    "#             print(profit)\n",
    "#         except:\n",
    "#             pass\n",
    "#     portfolio_profit\n",
    "\n",
    "\n",
    "#     profit_list_index = 0\n",
    "\n",
    "    profit_yah_index = yf.download(index)['Adj Close'].fillna(method='backfill')[str(int(start)+i)] \n",
    "    profit_index = (profit_yah_index[-1]-profit_yah_index[0])/profit_yah_index[0]\n",
    "\n",
    "    \n",
    "\n",
    "    print('Год начальный')\n",
    "    print(start_hayoo)\n",
    "    \n",
    "#     profit_list_index = profit_index\n",
    "\n",
    "    portfolio_profit_final.append(np.mean(portfolio_profit)*100)\n",
    "    index_profit_final.append(profit_index*100)\n",
    "\n",
    "#     returnezzz = pd.DataFrame()\n",
    "#     returnezzz['Portfolio'] = [np.mean(portfolio_profit)*100]\n",
    "#     returnezzz['Index'] = [profit_list_index*100]\n",
    "\n",
    "    print(portfolio_profit_final)\n",
    "\n",
    "    print('top_rated_company')\n",
    "    print(top_rated_company)\n",
    "    print('low_rated_company')\n",
    "    print(low_rated_company)\n",
    "       \n",
    "\n",
    "    print('Max DD')\n",
    "    print(max_dd_list)\n",
    "    print(np.min(max_dd_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Страна</th>\n",
       "      <th>Начало периода</th>\n",
       "      <th>Дходность с ребалансировкой портфеля</th>\n",
       "      <th>Дходность Индекса</th>\n",
       "      <th>Max DD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Индия</td>\n",
       "      <td>2015</td>\n",
       "      <td>154.315844</td>\n",
       "      <td>24.126498</td>\n",
       "      <td>-86.812754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Страна Начало периода  Дходность с ребалансировкой портфеля  \\\n",
       "0  Индия           2015                            154.315844   \n",
       "\n",
       "   Дходность Индекса     Max DD  \n",
       "0          24.126498 -86.812754  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnez_cum_port = pd.DataFrame(portfolio_profit_final).dropna()  \n",
    "returnez_cum_index = pd.DataFrame(index_profit_final).dropna()  \n",
    "\n",
    "returnez = pd.DataFrame()\n",
    "\n",
    "returnez['Страна'] = [LIST]\n",
    "returnez['Начало периода'] = [cheked_year]\n",
    "returnez['Дходность с ребалансировкой портфеля'] = ((1 + (returnez_cum_port/100)).cumprod().iloc[-1]-1)*100\n",
    "returnez['Дходность Индекса'] = ((1 + (returnez_cum_index/100)).cumprod().iloc[-1]-1)*100\n",
    "returnez['Max DD'] = [np.min(max_dd_list)]\n",
    "\n",
    "# gc = gd.service_account(filename='Seetzzz-1cb93f64d8d7.json')\n",
    "# worksheet = gc.open(\"Тесты бэктестинга\").worksheet('Мульти-фактор2')\n",
    "\n",
    "# worksheet.update('A20', [returnez.columns.tolist()] + returnez.values.tolist())\n",
    "\n",
    "returnez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_str_returns_bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
